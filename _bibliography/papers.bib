---
---

@string{aps = {American Physical Society,}}


@inproceedings{bretin_co-existing_2022,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '22},
	title = {Co-existing {With} a {Drone}: {Using} {Virtual} {Reality} to {Investigate} the {Effect} of the {Drone}’s {Height} and {Cover} {Story} on {Proxemic} {Behaviours}},
	isbn = {978-1-4503-9156-6},
	shorttitle = {Co-existing {With} a {Drone}},
	url = {https://doi.org/10.1145/3491101.3519750},
	doi = {10.1145/3491101.3519750},
	abstract = {While a growing body of literature has begun to examine proxemics in light of human–robot interactions, it is unclear how insights gained from human–human or human–robot interaction (HRI) apply during human–drone interactions (HDI). Understanding why and how people locate themselves around drones is thus critical to ensure drones are socially acceptable. In this paper, we present a proxemic user study (N=45) in virtual reality focusing on 1) the impact of the drone’s height and 2) the type of cover story used to introduce the drone (framing) on participants’ proxemic preferences. We found that the flying height has a statistically significant effect on the preferred interpersonal distance, whereas no evidence was found related to how the drone was framed. While results also highlight the value of using Virtual Reality for HDI experiments, further research must be carried out to investigate how these findings translate from the virtual to the real world.},
	urldate = {2022-07-26},
	booktitle = {Extended {Abstracts} of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bretin, Robin and Cross, Emily S. and Khamis, Mohamed},
	month = apr,
	year = {2022},
	keywords = {Human-Drone Interaction, Virtual Reality, Framing, Proxemic, Social Drone},
	pages = {1--9},
	preview={coexCHI_page-0005.jpg},
}

@article{bretin_co-existing_2024,
	title = {Co-existing with {Drones}: {A} {Virtual} {Exploration} of {Proxemic} {Behaviours} and {Users}’ {Insights} on {Social} {Drones}},
	issn = {1875-4805},
	shorttitle = {Co-existing with {Drones}},
	url = {https://doi.org/10.1007/s12369-024-01111-7},
	doi = {10.1007/s12369-024-01111-7},
	abstract = {Numerous studies have investigated proxemics in the context of human–robot interactions, but little is known about whether these insights can be applied to human–drone interactions (HDI). As drones become more common in social settings, it is crucial to ensure they navigate in a socially acceptable and human-friendly way. Understanding how individuals position themselves around drones is vital to promote user well-being and drones’ social acceptance. However, real-world constraints and risks associated with drones flying in close proximity to participants have limited research in this field. Virtual reality is a promising alternative for investigating HDI, as prior research suggests. This paper presents a proxemic user study (N = 45) in virtual reality, examining how drone height and framing influence participants’ proxemic preferences. The study also explores participants’ perceptions of social drones and their vision for the future of flying robots. Our findings show that drone height significantly impacts participants’ preferred interpersonal distance, while framing had no significant effect. Thoughts on how participants envision social drones (e.g., interaction, design, applications) reveal interpersonal differences but also shows overall consistency over time. While the study demonstrates the value of using virtual reality for HDI experiments, further research is necessary to determine the generalizability of our findings to real-world HDI scenarios.},
	language = {en},
	urldate = {2024-03-17},
	journal = {International Journal of Social Robotics},
	author = {Bretin, Robin and Cross, Emily and Khamis, Mohamed},
	month = mar,
	year = {2024},
	keywords = {Virtual reality, Framing, Proxemic, Human–drone interaction, Social drone},
	selected={true},
	altmetric={true},
	dimensions={true},
	preview={coexIJSR.jpg},
}

@inproceedings{bretin_i_2023,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {“{Do} {I} {Run} {Away}?”: {Proximity}, {Stress} and {Discomfort} in {Human}-{Drone} {Interaction} in {Real} and {Virtual} {Environments}},
	isbn = {978-3-031-42283-6},
	shorttitle = {“{Do} {I} {Run} {Away}?},
	doi = {10.1007/978-3-031-42283-6_29},
	abstract = {Social drones are autonomous flying machines designed to operate in inhabited environments. Yet, little is known about how their proximity might impact people’s well-being. This knowledge is critical as drones are often perceived as potential threats due to their design (e.g., visible propellers, unpleasant noise) and capabilities (e.g., moving at high speed, surveillance). In parallel, Virtual Reality (VR) is a promising tool to study human–drone interactions. However, important questions remain as to whether VR is ecologically valid for exploring human–drone interactions. Here, we present a between-within subjects user study (N = 42) showing that participants’ stress significantly differs between different drone states and locations. They felt more comfortable when the drone retreated from their personal space. Discomfort and stress were strongly correlated with the perceived drone’s threat level. Similar findings were found across real and virtual environments. We demonstrate that drones’ behaviour and proximity can threaten peoples’ well-being and comfort, and propose evidence-based guidelines to mitigate these impacts.},
	language = {en},
	booktitle = {Human-{Computer} {Interaction} – {INTERACT} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Bretin, Robin and Khamis, Mohamed and Cross, Emily},
	editor = {Abdelnour Nocera, José and Kristín Lárusdóttir, Marta and Petrie, Helen and Piccinno, Antonio and Winckler, Marco},
	year = {2023},
	keywords = {Proxemics, Virtual Reality, Social Drones},
	pages = {525--551},
	selected={true},
	award= {{IFIP TC13 Pioneers' Award for Best Doctoral Student Paper}, {Reviewers’ Choice Best Paper Award}},
	altmetric={true},
	dimensions={true},
	preview={doirun_page-0008.jpg},
}

@inproceedings{fiani_big_2023,
	address = {New York, NY, USA},
	series = {{IDC} '23},
	title = {Big {Buddy}: {Exploring} {Child} {Reactions} and {Parental} {Perceptions} towards a {Simulated} {Embodied} {Moderating} {System} for {Social} {Virtual} {Reality}},
	isbn = {979-8-4007-0131-3},
	shorttitle = {Big {Buddy}},
	url = {https://doi.org/10.1145/3585088.3589374},
	doi = {10.1145/3585088.3589374},
	abstract = {Children experience new forms of harassment in Social Virtual Reality (VR), often inaccessible to parental oversight. We aimed to understand how an artificial intelligent moderator safeguarding children from harassment in social VR is perceived by children and parents, by introducing “Big Buddy”, a Wizard-of-Oz embodied AI-moderator. 43 children (aged 8-16) played a tower-block-construction game in a simulated Social VR classroom where fictitious competitors disrupted their game and, in experimental conditions where present, Big Buddy intervened. We measured children’s perceptions after the disruptions, towards Big Buddy, and the moderation actions it took. Children felt significantly less sad and safer when Big Buddy suspended the saboteur. Parents (n=17) noted Big Buddy’s usefulness and felt reassured but would remain in the supervision loop. We present the first empirical research of a VR-embodied AI-moderator with children’s and parents’ perspectives, and propose design directions for embodied AI-moderators in Social VR.},
	urldate = {2024-12-01},
	booktitle = {Proceedings of the 22nd {Annual} {ACM} {Interaction} {Design} and {Children} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Fiani, Cristina and Bretin, Robin and Mcgill, Mark and Khamis, Mohamed},
	month = jun,
	year = {2023},
	pages = {1--13},
	selected={true},
	altmetric={true},
	dimensions={true},
	preview={bigbuddyIDC_page-0006.jpg},
}

@inproceedings{watson_feet_2022,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '22},
	title = {The {Feet} in {Human}-{Centred} {Security}: {Investigating} {Foot}-{Based} {User} {Authentication} for {Public} {Displays}},
	isbn = {978-1-4503-9156-6},
	shorttitle = {The {Feet} in {Human}-{Centred} {Security}},
	url = {https://doi.org/10.1145/3491101.3519838},
	doi = {10.1145/3491101.3519838},
	abstract = {A large body of work investigated touch, mid-air, and gaze-based user authentication. However, little is known about authentication using other human body parts. In this paper, we investigate the idea of foot-based user authentication for public displays (e.g., ticket machines). We conducted a user study (N=13) on a virtual prototype, FeetAuth, on which participants use their dominant foot to rotate through PIN elements (0–9) that are augmented along a circular layout using augmented reality (AR) technology. We investigate FeetAuth\&nbsp; in combination with three different layouts: Floor-based, Spatial, and Egocentric, finding that Floor-based\&nbsp;FeetAuth\&nbsp;resulted in the highest usability with 4-digit PIN entry as fast as M=6.71 (SD=0.67). Participants perceived foot-based authentication as socially acceptable and highlighted its accessibility. Our investigation of foot-based authentication paves the way for further studies on the use of the human body for user authentication.},
	urldate = {2024-12-01},
	booktitle = {Extended {Abstracts} of the 2022 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Watson, Kieran and Bretin, Robin and Khamis, Mohamed and Mathis, Florian},
	month = apr,
	year = {2022},
	pages = {1--9},
	preview={feetHCI_page-0002.jpg},
}

@inproceedings{fiani_pikachu_2024,
	address = {New York, NY, USA},
	series = {{CHI} '24},
	title = {"{Pikachu} would electrocute people who are misbehaving": {Expert}, {Guardian} and {Child} {Perspectives} on {Automated} {Embodied} {Moderators} for {Safeguarding} {Children} in {Social} {Virtual} {Reality}},
	isbn = {979-8-4007-0330-0},
	shorttitle = {"{Pikachu} would electrocute people who are misbehaving"},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642144},
	doi = {10.1145/3613904.3642144},
	abstract = {Automated embodied moderation has the potential to create safer spaces for children in social VR, providing a protective figure that takes action to mitigate harmful interactions. However, little is known about how such moderation should be employed in practice. Through interviews with 16 experts in online child safety and psychology, and workshops with 8 guardians and 13 children, we contribute a comprehensive overview of how Automated Embodied Moderators (AEMs) can safeguard children in social VR. We explore perceived concerns, benefits and preferences across the stakeholder groups and gather first-of-their-kind recommendations and reflections around AEM design. The results stress the need to adapt AEMs to children, whether victims or harassers, based on age and development, emphasising empowerment, psychological impact and humans/guardians-in-the-loop. Our work provokes new participatory design-led directions to consider in the development of AEMs for children in social VR taking child, guardian, and expert insights into account.},
	urldate = {2024-12-01},
	booktitle = {Proceedings of the 2024 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fiani, Cristina and Bretin, Robin and Macdonald, Shaun Alexander and Khamis, Mohamed and Mcgill, Mark},
	month = may,
	year = {2024},
	pages = {1--23},
	preview={pikachu_page-0001.jpg},
}

@inproceedings{fiani_big_2023-1,
	address = {New York, NY, USA},
	series = {{CHI} {EA} '23},
	title = {Big {Buddy}: {A} {Simulated} {Embodied} {Moderating} {System} to {Mitigate} {Children}’s {Reaction} to {Provocative} {Situations} within {Social} {Virtual} {Reality}},
	isbn = {978-1-4503-9422-2},
	shorttitle = {Big {Buddy}},
	url = {https://doi.org/10.1145/3544549.3585840},
	doi = {10.1145/3544549.3585840},
	abstract = {The use of social Virtual Reality (VR) among children is increasing, but with it comes new forms of harassment that can be difficult for parents to monitor. To address this issue, we have developed "Big Buddy", a prototype AI-moderator that aims to safeguard children from potential harassment in social VR. We conducted a study in which 43 children (aged 8-16) participated in a simulated social VR classroom, with fictitious competitors disrupting their game. When Big Buddy intervened, the children reported feeling significantly less negative emotions and felt safer. This is the first study to empirically examine the use of an embodied AI-moderator in social VR from the perspective of children, and it provides important insights for designing AI-moderators in social VR.},
	urldate = {2024-12-01},
	booktitle = {Extended {Abstracts} of the 2023 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Fiani, Cristina and Bretin, Robin and McGill, Mark and Khamis, Mohamed},
	month = apr,
	year = {2023},
	pages = {1--7},
	preview={bigbuddyCHI_page-0006.jpg},
}

@misc{xu_understanding_2024,
	title = {Understanding {Dynamic} {Human}-{Robot} {Proxemics} in the {Case} of {Four}-{Legged} {Canine}-{Inspired} {Robots}},
	url = {http://arxiv.org/abs/2302.10729},
	arxiv = {10.48550/arXiv.2302.10729},
	abstract = {Recently, quadruped robots have been well developed with potential applications in different areas, such as care homes, hospitals, and other social areas. To ensure their integration in such social contexts, it is essential to understand people's proxemic preferences around such robots. In this paper, we designed a human-quadruped-robot interaction study (N = 32) to investigate the effect of 1) different facing orientations and 2) the gaze of a moving robot on human proxemic distance. Our work covers both static and dynamic interaction scenarios. We found a statistically significant effect of both the robot's facing direction and its gaze on preferred personal distances. The distances humans established towards certain robot behavioral modes reflect their attitudes, thereby guiding the design of future autonomous robots.},
	urldate = {2024-12-01},
	publisher = {arXiv},
	author = {Xu, Xiangmin and Li, Emma Liying and Khamis, Mohamed and Zhao, Guodong and Bretin, Robin},
	month = sep,
	year = {2024},
	note = {arXiv:2302.10729},
	keywords = {Computer Science - Human-Computer Interaction},
	preview={canineProx_page-0006.jpg},
}

@inproceedings{macdonald_evaluating_2024,
	title = {Evaluating {Transferable} {Emotion} {Expressions} for {Zoomorphic} {Social} {Robots} using {VR} {Prototyping}‘},
	url = {https://ieeexplore.ieee.org/abstract/document/10765384},
	doi = {10.1109/ISMAR62088.2024.00125},
	abstract = {Zoomorphic robots have the potential to offer companionship and well-being as accessible, low-maintenance alternatives to pet ownership. Many such robots, however, feature limited emotional expression, restricting their potential for rich affective relationships with everyday domestic users. Additionally, exploring this design space using hardware prototyping is obstructed by physical and logistical constraints. We leveraged virtual reality rapid prototyping with passive haptic interaction to conduct a broad mixed-methods evaluation of emotion expression modalities and participatory prototyping of multimodal expressions. We found differences in recognisability, effectiveness and user empathy between modalities while highlighting the importance of facial expressions and the benefits of combining animal-like and unambiguous modalities. We use our findings to inform promising directions for the affective zoomorphic robot design and potential implementations via hardware modification or augmented reality, then discuss how VR prototyping makes this field more accessible to designers and researchers.},
	urldate = {2024-12-01},
	booktitle = {2024 {IEEE} {International} {Symposium} on {Mixed} and {Augmented} {Reality} ({ISMAR})},
	author = {Macdonald, Shaun and Bretin, Robin and ElSayed, Salma},
	month = oct,
	year = {2024},
	note = {ISSN: 2473-0726},
	keywords = {Robots, Social robots, Augmented reality, Virtual Reality, Human-Robot Interaction, Affective Computing, Emotion recognition, Hardware, Rapid prototyping, Space exploration, Tail, Text recognition, Text to speech, Zoomorphic Robots},
	pages = {1087--1096},
	preview={Evaluating_Transferable_Emotion_Expressions_for_Zoomorphic_Social_Robots_using_VR_Prototyping_page-0001.jpg},
}
